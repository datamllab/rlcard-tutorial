{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mia1996/rlcard-tutoirial/blob/master/random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBl4S8JARzX"
      },
      "source": [
        "\n",
        "\n",
        "# <a href='https://github.com/datamllab/rlcard'> <center> <img src='https://miro.medium.com/max/1000/1*_9abDpNTM9Cbsd2HEXYm9Q.png' width=500 class='center' /></a> \n",
        "\n",
        "## **Playing with Random Agents**\n",
        "This example shows how we can make an RLCard environment and interact with the environment. We use Leduc Hold'em environment as an example to walk through the process of installing RLCard and making an environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZbqww8VXSOx"
      },
      "source": [
        "\n",
        "First, we install RLcard and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ8CiXAJjQGi",
        "outputId": "8315136d-261c-43a1-a25e-26914e3e048d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install rlcard[torch]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rlcard[torch] in /usr/local/lib/python3.7/dist-packages (1.0.7)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (3.2.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (3.1.27)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (1.10.0+cu111)\n",
            "Requirement already satisfied: gitdb2 in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (4.0.2)\n",
            "Requirement already satisfied: gitdb>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb2->rlcard[torch]) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb>=4.0.1->gitdb2->rlcard[torch]) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython->rlcard[torch]) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->rlcard[torch]) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we import rlcard and a random agent, which moves randomly, to interact with the environment."
      ],
      "metadata": {
        "id": "ZgY-v-eKrVSb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_A_Br3Jj0xW"
      },
      "source": [
        "import rlcard\n",
        "from rlcard.agents import RandomAgent"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ltkfinYmiU"
      },
      "source": [
        "Now we can create a Leduc Hold'em environment by simply passing `leduc-holdem` to the `make` method of the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_8Kuf47kghG"
      },
      "source": [
        "env = rlcard.make(\"leduc-holdem\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything about Leduc Hold'em is wrapped in `env`. Now let's take a look at what we have in the environment."
      ],
      "metadata": {
        "id": "c-fBQP6MsZBB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bqfMncnJTYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0866a7-8199-4d4f-d38e-a2145f7504ac"
      },
      "source": [
        "print(\"Number of actions:\", env.num_actions)\n",
        "print(\"Number of players:\", env.num_players)\n",
        "print(\"Shape of state:\", env.state_shape)\n",
        "print(\"Shape of action:\", env.action_shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of actions: 4\n",
            "Number of players: 2\n",
            "Shape of state: [[36], [36]]\n",
            "Shape of action: [None, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can know that the Leduc Hold'em environment is a 2-player game with 4 possible actions. The state (which means all the information when can observe at a specific step) is of the shape of 36. There is no action features. Differnt environments have different characteristics. You can try other environments as well. For example, you can try Dou Dizhu."
      ],
      "metadata": {
        "id": "0tq4Y7bfs7Zh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhgGYyick13x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb95e0f-b0cd-4913-f61a-c9f92e275ab8"
      },
      "source": [
        "env_doudizhu = rlcard.make(\"doudizhu\")\n",
        "print(\"Number of actions:\", env_doudizhu.num_actions)\n",
        "print(\"Number of players:\", env_doudizhu.num_players)\n",
        "print(\"Shape of state:\", env_doudizhu.state_shape)\n",
        "print(\"Shape of action:\", env_doudizhu.action_shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of actions: 27472\n",
            "Number of players: 3\n",
            "Shape of state: [[790], [901], [901]]\n",
            "Shape of action: [[54], [54], [54]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This environment is much more complex with a large number of possible actions and very large state and action spaces. Despite these chllenges, RLCard has implemented the algorithm [DMC](https://github.com/kwai/DouZero) in [DouZero](https://arxiv.org/abs/2106.06135), which leads to strong agents. We will show how to train strong Dou Dizhu AI with RLCard in later tutorials."
      ],
      "metadata": {
        "id": "YmHHx63OtveO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's just interact with the environment with random agents. We first tell the environment we want to use random agents to interact by using `set_agents`."
      ],
      "metadata": {
        "id": "FNw8nykwuqp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RandomAgent(num_actions=env.num_actions)\n",
        "env.set_agents([agent for _ in range(env.num_players)])"
      ],
      "metadata": {
        "id": "vNx-cm-Gu_OU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to make interactions with `run`."
      ],
      "metadata": {
        "id": "1x4z9QS9vJOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trajectories, player_wins = env.run(is_training=False)"
      ],
      "metadata": {
        "id": "GQUMYyObvTlc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, `is_training` indicates we use the `eval_step` method of the random agent to interact. If `is_training` is `True`, it will instead use `step` instead. This interface is designed for some certain reinforcement learning algorithms that have differnce behaviors in training and evaluation. `trajectories` is the collected data. Let's print it out!"
      ],
      "metadata": {
        "id": "X51bJQg7vYqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(trajectories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyR-66zJv7by",
        "outputId": "c33892f3-1b73-4dda-f134-6b4ef9a02a6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'legal_actions': OrderedDict([(0, None), (1, None), (2, None)]), 'obs': array([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0.]), 'raw_obs': {'hand': 'HK', 'public_card': None, 'all_chips': [2, 4], 'my_chips': 2, 'legal_actions': ['call', 'raise', 'fold'], 'current_player': 0}, 'raw_legal_actions': ['call', 'raise', 'fold'], 'action_record': [(1, 'raise'), (0, 'fold')]}, 2, {'legal_actions': OrderedDict([(1, None), (2, None), (3, None)]), 'obs': array([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0.]), 'raw_obs': {'hand': 'HK', 'public_card': None, 'all_chips': [2, 4], 'my_chips': 2, 'legal_actions': ['raise', 'fold', 'check'], 'current_player': 1}, 'raw_legal_actions': ['raise', 'fold', 'check'], 'action_record': [(1, 'raise'), (0, 'fold')]}], [{'legal_actions': OrderedDict([(0, None), (1, None), (2, None)]), 'obs': array([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0.]), 'raw_obs': {'hand': 'SJ', 'public_card': None, 'all_chips': [2, 1], 'my_chips': 1, 'legal_actions': ['call', 'raise', 'fold'], 'current_player': 1}, 'raw_legal_actions': ['call', 'raise', 'fold'], 'action_record': [(1, 'raise'), (0, 'fold')]}, 1, {'legal_actions': OrderedDict([(1, None), (2, None), (3, None)]), 'obs': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0.]), 'raw_obs': {'hand': 'SJ', 'public_card': None, 'all_chips': [2, 4], 'my_chips': 4, 'legal_actions': ['raise', 'fold', 'check'], 'current_player': 1}, 'raw_legal_actions': ['raise', 'fold', 'check'], 'action_record': [(1, 'raise'), (0, 'fold')]}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These consist of observations and actions of the two players. They can used for training reinforcement learning agents. Let's print out `player_wins` as well."
      ],
      "metadata": {
        "id": "zOYUhs_nwCRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(player_wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSfpMIhFwf_O",
        "outputId": "d69070b5-24e4-4d62-d8a7-b895beda5a27"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.  1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These mean the second player wins and gets 1 point. These are all the interfaces we need to know to train most of the reinforcment learning agents! We will show how to do this in later tutorials."
      ],
      "metadata": {
        "id": "2DANqImrw3Nb"
      }
    }
  ]
}